{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /home/marzi/anaconda3/lib/python3.8/site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/marzi/anaconda3/lib/python3.8/site-packages (from tweepy) (1.15.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marzi/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/marzi/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/marzi/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/marzi/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /home/marzi/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install gensim\n",
    "!pip install pyldavis\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!pip install textstat\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import nltk, time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'QxA50wplG6jBYLpNMBDJCYjcH'\n",
    "consumer_secret= 'Js4FSedtS30D6HEItelpIc3mNtu2NgFKsO04rbRjj73WykIlGn'\n",
    "access_token= '1329496543779573760-tOMLouKs116LWwgXZF7oZdtGU2I8an'\n",
    "access_token_secret= 'c2cPhglkr87O15nLPkxZlNVMPNDwFqf6oZRDiD8hUpPR6'\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"koronawirus\"\n",
    "date_since = \"2020-01-01\"\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              tweet_mode='extended',\n",
    "              lang=\"pl\",\n",
    "              since=date_since).items(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tweet = []\n",
    "for tweet in tweets:\n",
    "    list_tweet.append(tweet.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/2\\nKoronawirus wciąż wstrzymuje ruch w halac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @sondypolityczne: #sonda | czy protesty #St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/2\\nKoronawirus wciąż wstrzymuje ruch w halac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @agnieszkawolsk9: Kto i kiedy wybrał Polskę...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @agnieszkawolsk9: Kto i kiedy wybrał Polskę...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  2/2\\nKoronawirus wciąż wstrzymuje ruch w halac...\n",
       "1  RT @sondypolityczne: #sonda | czy protesty #St...\n",
       "2  1/2\\nKoronawirus wciąż wstrzymuje ruch w halac...\n",
       "3  RT @agnieszkawolsk9: Kto i kiedy wybrał Polskę...\n",
       "4  RT @agnieszkawolsk9: Kto i kiedy wybrał Polskę..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pl_core_news_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granice granica SUBST\n",
      "mojego mój ADJ\n",
      "języka język SUBST\n",
      "oznaczają oznaczać FIN\n",
      "granice granica SUBST\n",
      "mojego mój ADJ\n",
      "świata świat SUBST\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Granice mojego języka oznaczają granice mojego świata\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pl\" id=\"70f1c30474524ae1b4538b68c8cb1b80-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Granice</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">mojego</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">języka</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">oznaczają</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">granice</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mojego</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">świata</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70f1c30474524ae1b4538b68c8cb1b80-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70f1c30474524ae1b4538b68c8cb1b80-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70f1c30474524ae1b4538b68c8cb1b80-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70f1c30474524ae1b4538b68c8cb1b80-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70f1c30474524ae1b4538b68c8cb1b80-0-2\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70f1c30474524ae1b4538b68c8cb1b80-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,266.5 L403.0,254.5 387.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70f1c30474524ae1b4538b68c8cb1b80-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70f1c30474524ae1b4538b68c8cb1b80-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70f1c30474524ae1b4538b68c8cb1b80-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70f1c30474524ae1b4538b68c8cb1b80-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-70f1c30474524ae1b4538b68c8cb1b80-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-70f1c30474524ae1b4538b68c8cb1b80-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blokowanie unijnego budżetu i funduszu odbudowy po #koronawirus to nie tylko polityczne awanturnictwo. To odbierani… https://t.co/jovTkHbTfF'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tweet[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(list_tweet[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    \"\"\"Function that tokenizes text\"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return word_tokenize(text)\n",
    "In [4]:\n",
    "def clean_up(data):\n",
    "    \"\"\"Function that cleans up the data into a shape that can be further used for modeling\"\"\"\n",
    "    english = data[data['lang']=='en'] # extract only tweets in english language\n",
    "    english.drop_duplicates() # drop duplicate tweets\n",
    "    english['text'].dropna(inplace=True) # drop any rows with missing tweets\n",
    "    tokenized = english['text'].apply(custom_tokenize) # Tokenize tweets\n",
    "    lower_tokens = tokenized.apply(lambda x: [t.lower() for t in x]) # Convert tokens into lower case\n",
    "    alpha_only = lower_tokens.apply(lambda x: [t for t in x if t.isalpha()]) # Remove punctuations\n",
    "    no_stops = alpha_only.apply(lambda x: [t for t in x if t not in stopwords.words('english')]) # remove stop words\n",
    "    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='rt']) # remove acronym \"rt\"\n",
    "    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='https']) # remove acronym \"https\"\n",
    "    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='twitter']) # remove the word \"twitter\"\n",
    "    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='retweet']) # remove the word \"retweet\"\n",
    "    return no_stops\n",
    "In [5]:\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "use_cols = ['text', 'lang']\n",
    "path = 'C:\\\\Users\\\\Bauyrjan.Jyenis\\\\Capstone3\\\\all_tweets.csv'\n",
    "data_iterator = pd.read_csv(path, usecols=use_cols, chunksize=50000)\n",
    "chunk_list = []\n",
    "for data_chunk in data_iterator:\n",
    "    filtered_chunk = clean_up(data_chunk)\n",
    "    chunk_list.append(filtered_chunk)\n",
    "tidy_data = pd.concat(chunk_list)\n",
    "end = time.time()\n",
    "total=end-start\n",
    "total #46min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tweets.str.len().hist(grid=False, figsize=(9,7), bins=20)\n",
    "_ = plt.xlabel('Number of words')\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Number of words in each tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Create a Dictionary from the tweets\n",
    "start = time.time()\n",
    "\n",
    "dictionary = Dictionary(tweets)\n",
    "\n",
    "end = time.time()\n",
    "total = end-start\n",
    "total\n",
    "start = time.time()\n",
    "corpus = tweets.apply(lambda x: dictionary.doc2bow(x))\n",
    "\n",
    "end = time.time()\n",
    "total = end-start\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, itertools\n",
    "start = time.time()\n",
    "\n",
    "total_word_count = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id]+=word_count\n",
    "    \n",
    "end = time.time()\n",
    "total = end-start\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sorted_bow = sorted(total_word_count.items(), key=lambda kv :kv[1], reverse=True)\n",
    "words = []\n",
    "word_counts = []\n",
    "for word_id, word_count in sorted_bow[:20]:\n",
    "    words.append(dictionary.get(word_id)) \n",
    "    word_counts.append(word_count)\n",
    "    \n",
    "end = time.time()\n",
    "total = end-start\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sns.barplot(x=word_counts, y=words)\n",
    "fig = plt.xlabel(\"Count\")\n",
    "fig = plt.ylabel('Words')\n",
    "fig = plt.title('20 Most Frequent Words Across All Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "start = time.time()\n",
    "\n",
    "bigrams = tweets.apply(lambda x: ngrams(x,2))\n",
    "\n",
    "end = time.time()\n",
    "total = end-start\n",
    "total\n",
    "\n",
    "bi_grams = bigrams.apply(lambda x: list(x))\n",
    "top_20_bigrams = Counter(itertools.chain.from_iterable(bi_grams)).most_common(20)\n",
    "top_bigram = pd.DataFrame(top_20_bigrams, columns=['bigram', 'frequency'])\n",
    "top_bigram.set_index('bigram', inplace=True)\n",
    "_ = plt.figure(figsize=(12,9))\n",
    "_ = sns.barplot(x=top_bigram.frequency, y=top_bigram.index, orient='h')\n",
    "_ = plt.xlabel('Frequency')\n",
    "_ = plt.ylabel('Bigrams')\n",
    "_ = plt.title('Top 20 Bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        random_state=1)\n",
    "    wordcloud=wordcloud.generate(str(data))\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
